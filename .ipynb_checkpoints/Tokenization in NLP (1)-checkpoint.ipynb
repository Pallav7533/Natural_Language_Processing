{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0bfa4f",
   "metadata": {},
   "source": [
    "# NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0d9eb",
   "metadata": {},
   "source": [
    "# corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3f674b",
   "metadata": {},
   "source": [
    "A large collection of documents used for training NLP models. The corpus provides the data from which the vocabulary is built and upon which the models are trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f21e6d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:  ['I love NLP.', 'NLP is fun.']\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"I love NLP.\", \"NLP is fun.\"]\n",
    "print(\"Corpus: \", corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2289a585",
   "metadata": {},
   "source": [
    "# Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f7d1f9",
   "metadata": {},
   "source": [
    "Definition: A single piece of text, such as an article, a paragraph, or a sentence. It is usually the unit of analysis in text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8172df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document:  I love NLP.\n"
     ]
    }
   ],
   "source": [
    "document = \"I love NLP.\"\n",
    "print(\"Document: \", document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c50881",
   "metadata": {},
   "source": [
    "# words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa33a7",
   "metadata": {},
   "source": [
    "Definition: The basic units of language, representing distinct meanings. Words can be in different forms, such as singular, plural, verbs, nouns, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a50e68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  ['cat', 'running', 'beautiful']\n"
     ]
    }
   ],
   "source": [
    "words = [\"cat\", \"running\", \"beautiful\"]\n",
    "print(\"Words: \", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44140e33",
   "metadata": {},
   "source": [
    "# Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65a6c45",
   "metadata": {},
   "source": [
    "Definition: Tokens are individual pieces of a sentence or text that have been segmented, often corresponding to words, punctuation, or other meaningful elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32b66f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  ['I', 'love', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"I love NLP.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "print(\"Tokens: \", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ebc71c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013d2ae8",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03132e17",
   "metadata": {},
   "source": [
    "Definition: The set of unique tokens found in a corpus. This represents the words and symbols that the model or algorithm has been trained to recognize and understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a797cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52ba4599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  {'love', 'NLP', 'I', 'is', 'fun'}\n"
     ]
    }
   ],
   "source": [
    "documents = [\"I love NLP\", \"NLP is fun\"]\n",
    "\n",
    "tokenized_documents = [word_tokenize(doc) for doc in documents]\n",
    "vocabulary = set(token for doc in tokenized_documents for token in doc)\n",
    "print(\"Vocabulary: \", vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ad6179",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048f6ed",
   "metadata": {},
   "source": [
    "Definition: The process of breaking down a text into smaller units, typically tokens. Tokenization is a crucial preprocessing step in NLP as it transforms raw text into a format that can be analyzed and processed by algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c4aec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af37cb7",
   "metadata": {},
   "source": [
    "# Types of Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8880fa30",
   "metadata": {},
   "source": [
    "# word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd966ab",
   "metadata": {},
   "source": [
    "Breaks text into individual words or tokens based on whitespace or punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf615d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization:  ['I', 'love', 'NLP', '.', 'NLP', 'is', 'fun', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"I love NLP. NLP is fun!\"\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Word Tokenization: \", word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dbe1cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'to',\n",
       " ',',\n",
       " 'my',\n",
       " 'mastery',\n",
       " 'map',\n",
       " 'of',\n",
       " '#',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'Do',\n",
       " 'follow',\n",
       " 'me',\n",
       " '!']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "corpus = \"\"\" Welcome to, my mastery map of #NLP. \n",
    "Do follow me ! \"\"\"\n",
    "\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810f9c4",
   "metadata": {},
   "source": [
    "word_tokenize v/s wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14639fb5",
   "metadata": {},
   "source": [
    "# wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b84a9",
   "metadata": {},
   "source": [
    "Function: Splits text into words and separates all punctuation characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdb654b",
   "metadata": {},
   "source": [
    "# word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd147f",
   "metadata": {},
   "source": [
    "Function: Splits text into words while treating punctuation more contextually, often keeping contractions and other linguistic elements together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9d239f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "wordpunct_tokenize: ['Don', \"'\", 't', 'go', '.', 'Hello', ',', 'world', '!']\n",
      "2.\n",
      "word_tokenize: ['Do', \"n't\", 'go', '.', 'Hello', ',', 'world', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "text = \"Don't go. Hello, world!\"\n",
    "\n",
    "print('1.')\n",
    "\n",
    "# Using wordpunct_tokenize\n",
    "\n",
    "tokens_wp = wordpunct_tokenize(text)\n",
    "print(\"wordpunct_tokenize:\", tokens_wp)\n",
    "\n",
    "print('2.')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Don't go. Hello, world!\"\n",
    "\n",
    "# Using word_tokenize\n",
    "\n",
    "tokens_wt = word_tokenize(text)\n",
    "print(\"word_tokenize:\", tokens_wt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f6df21",
   "metadata": {},
   "source": [
    "# Sentence Tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab542849",
   "metadata": {},
   "source": [
    "Splits text into individual sentences based on punctuation marks like periods, exclamation points, or question marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd05c2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization: ['I love NLP.', 'NLP is fun!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"I love NLP. NLP is fun!\"\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "\n",
    "print(\"Sentence Tokenization:\", sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b342573",
   "metadata": {},
   "source": [
    "# Whitespace Tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418b1dc6",
   "metadata": {},
   "source": [
    "Splits text based on whitespace characters like spaces, tabs, or newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b9253b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokenization:  ['I', 'love', 'NLP.', 'NLP', 'is', 'fun!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "text = \"I love NLP. NLP is fun!\"\n",
    "\n",
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "whitespace_tokens = whitespace_tokenizer.tokenize(text)\n",
    "\n",
    "print(\"Whitespace Tokenization: \", whitespace_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd8973",
   "metadata": {},
   "source": [
    "# Regular Expression Tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ceee5d",
   "metadata": {},
   "source": [
    "Tokenizes text based on specified patterns using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26f00a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Expression Tokenization:  ['I', 'love', 'NLP', 'NLP', 'is', 'fun']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "text = \"I love NLP. NLP is fun!\"\n",
    "\n",
    "pattern = r'\\w+'\n",
    "regexp_tokenizer = RegexpTokenizer(pattern)\n",
    "regexp_tokens = regexp_tokenizer.tokenize(text)\n",
    "\n",
    "print(\"Regular Expression Tokenization: \", regexp_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb9ae1",
   "metadata": {},
   "source": [
    "# NGram Tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd4996",
   "metadata": {},
   "source": [
    "Creates tokens by combining N consecutive words from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "564250af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ['I', 'love', 'NLP', '.', 'NLP', 'is', 'fun', '!']\n",
      "N-Gram Tokenization (Bigrams):  [('I', 'love'), ('love', 'NLP'), ('NLP', '.'), ('.', 'NLP'), ('NLP', 'is'), ('is', 'fun'), ('fun', '!')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "text = \"I love NLP. NLP is fun!\"\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\" \",word_tokens)\n",
    "\n",
    "n = 2\n",
    "ngram_tokens = list(ngrams(word_tokens, n))\n",
    "print(\"N-Gram Tokenization (Bigrams): \", ngram_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47d25c5",
   "metadata": {},
   "source": [
    "# Custom Tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d921b7",
   "metadata": {},
   "source": [
    "Tailors tokenization rules based on specific requirements or domain-specific knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55a7990c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Tokenization :  ['MRI', 'brain', 'COPD', 'lung']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def custom_tokenize(text):\n",
    "    medical_terms_pattern = r'(?:\\b(?:heart|lung|brain)\\b)|(?:\\b(?:COPD|MRI|ECG)\\b)'\n",
    "    tokens = re.findall(medical_terms_pattern, text, flags=re.IGNORECASE)\n",
    "    return tokens\n",
    "medical_text = \"The patient underwent an MRI scan to examine the brain. COPD is a chronic lung disease.\"\n",
    "custom_tokens = custom_tokenize(medical_text)\n",
    "print(\"Custom Tokenization : \", custom_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf79b6",
   "metadata": {},
   "source": [
    "# Treebank Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78896c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'to',\n",
       " ',',\n",
       " 'my',\n",
       " 'mastery',\n",
       " 'map',\n",
       " 'of',\n",
       " '#',\n",
       " 'NLP.',\n",
       " 'Do',\n",
       " 'follow',\n",
       " 'me',\n",
       " '!']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "TBWT = TreebankWordTokenizer()\n",
    "\n",
    "corpus = \"\"\" Welcome to, my mastery map of #NLP. \n",
    "Do follow me ! \"\"\"\n",
    "\n",
    "TBWT.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbec5612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
